---
title: "Scraping the news"
categories: R
date: '2020-12-15'
image:
  preview_only: yes
share: false
summary: Scraping data from Memeorandum.com to analyze political headlines.
tags: ["r", "visualization", "rvest", "web-scraping"]

authors: 
- admin
---

<link href="{{< relref "post/memeorandum-scraping/index.html" >}}index_files/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="{{< relref "post/memeorandum-scraping/index.html" >}}index_files/anchor-sections/anchor-sections.js"></script>


<p>We live in an age where the media you consume — and how you consume it — says a lot about you. If I told you that I read a lot of <em>The New York Times</em>, <em>Politico</em>, <em>The New Republic</em>, and <em>Vox</em>, you might reasonably guess something about my politics and interests. If I told you that I checked Twitter between ten and ten thousand times a day, that might also tell you something about me. But what if I told you the first place I look for news is <a href="https://www.memeorandum.com/"><em>Memeorandum.com</em></a>?</p>
<div class="figure">
<img src="snapshot.png" alt="The front page of Memeorandum at the time this post was originally published." />
<p class="caption"><em>The front page of Memeorandum at the time this post was originally published.</em></p>
</div>
<p>Chances are you've never heard of it, but it is in fact the place I've turned to most often over the last few years to get a bird's-eye view of the news I'm interested in. <em>Memeorandum</em> is an un-editorialized news aggregator that crawls the web to find the stories that are driving the news cycle, particularly in American politics. Its algorithm ingests news from all across the political spectrum, meaning you'll be able to find coverage of the same topics from both <em>Breitbart</em> and <em>Mother Jones</em>, or <em>The Daily Mail</em> and <em>The Washington Post</em>. If you can grow to appreciate the site's early-Internet aesthetic and slightly behind-the-times name (it was created in 2004), you'll be hard-pressed to find a better aggregator for consuming American news.</p>
<p>Nate Silver has referenced <em>Memeorandum</em> a few times in his work, and <a href="https://fivethirtyeight.com/features/how-donald-trump-hacked-the-media/">gave a nice explanation of the site here</a>:</p>
<blockquote>
<p>The site uses an algorithm to determine which stories are leading political coverage on the Internet; the details of the calculation are somewhat opaque, but a lot of it is based on which stories are being linked to by other news organizations and what themes are commonly recurring among different news outlets. Simply put, Memeorandum is a good indicator of what stories journalists are talking about.</p>
</blockquote>
<p>Another great feature of the site is its easily accessible archives. The site updates every five minutes with new articles, and the archive feature lets you navigate back to any five minute interval in the last sixteen years of <em>Memeorandum's</em> existence to find a snapshot of that moment's top stories. The result is a treasure trove of data that might tell us a thing or two about online political media and modern newsmaking, so in this post I'll walk through how I scraped the <em>Memeorandum</em> archives for 2020 and offer up a few simple analyses to demonstrate how the data can be used.</p>
<div id="loading-packages" class="section level2">
<h2>Loading packages</h2>
<p>The workhorse packages behind the work in this post are <code>rvest</code>, which handles the web-scraping and HTML-parsing I performed to collect the data, and <code>tidytext</code>, which has some useful functions for text analysis.</p>
<pre class="r"><code>library(tidyverse)
library(rvest)
library(lubridate)
library(tidytext)
library(ggtext)</code></pre>
</div>
<div id="scraping-the-archives" class="section level2">
<h2>Scraping the archives</h2>
<p>The <em>Memeorandum</em> main page is a simple static site that clusters headlines by topic and orders them by prevalence. Stories appear as either main items or related articles, and both the author and outlet are listed next to the headlines. The key to scraping <em>past</em> instances of the main page is to notice that the URL convention for archived posts follows a simple date-time format: <code>www.memeorandum.com/[YYMMDD]/h[hhmm]</code>. So the URL for a snapshot of the news on January 1st, 2010 at 5:00 PM EST would end with <code>/100101/h1700</code>.</p>
<p>I wrote a function that downloads the raw HTML from the site at 6:00 PM on a given date, then extracts the relevant headline, author, and outlet data from each story on the site at that moment. I was able to use <code>rvest::html_nodes()</code> and some trial-and-error to extract each story from one of two sections: the main items at the top of each topic cluster, or the sub-items comprised of related articles and discussion pieces.</p>
<pre class="r"><code>scrape_memeorandum &lt;- function(scrape_date) {
  # Convert date to relevant URL
  url &lt;- scrape_date %&gt;%
    format(&quot;%y%m%d&quot;) %&gt;%
    paste0(&quot;https://www.memeorandum.com/&quot;, ., &quot;/h1800&quot;) # snapshots at 6 PM EST
  # Read raw HTML
  raw &lt;- read_html(url)

  tibble( # scrape sub items
    byline = raw %&gt;%
      html_nodes(&quot;.lnkr cite&quot;) %&gt;%
      html_text(),
    headline = raw %&gt;%
      html_nodes(&quot;.lnkr a:nth-child(2)&quot;) %&gt;% # first child is redundant
      html_text(),
    scrape_date = scrape_date,
    section = &quot;sub&quot;
  ) %&gt;%
    bind_rows(
      tibble( # scrape main items
        byline = raw %&gt;%
          html_nodes(&quot;script ~ cite&quot;) %&gt;% # main items are structured differently, need subsequent selector
          html_text(),
        headline = raw %&gt;%
          html_nodes(&quot;strong a&quot;) %&gt;%
          html_text(),
        scrape_date = scrape_date,
        section = &quot;main&quot;
      )
    ) %&gt;%
    transmute(
      scrape_date,
      outlet = if_else( # parse byline into outlet/author
        str_detect(byline, &quot;\\s\\/\\s&quot;),
        str_extract(byline, &quot;(?&lt;=\\s\\/\\s).+(?=\\:)&quot;),
        str_extract(byline, &quot;.+(?=\\:)&quot;)
      ),
      author = str_extract(byline, &quot;.+(?=\\s\\/\\s)&quot;),
      headline,
      section
    )
}</code></pre>
<p>Once I had this working I iterated through each day of the year so far, pausing for 10 seconds between each visit to <em>Memeorandum</em> out of consideration for their servers. I stored each day's worth of data separately because if my loop failed spuriously, I didn't want to risk losing data and having to re-scrape dates.</p>
<p>{{% callout note %}}</p>
<p>I was not able to find any guidance from <em>Memeorandum</em> about their terms of use or scraping limits, so I tried to follow the basic etiquette <a href="https://benbernardblog.com/web-scraping-and-crawling-are-perfectly-legal-right/">described here</a>. If you plan on scraping <em>Memeorandum</em> as well, be conservative and be considerate. And if you take issue with my approach, please <a href="mailto::bchangsorensen@gmail.com">let me know</a>!</p>
<p>{{% /callout %}}</p>
<pre class="r"><code>scrape_dates &lt;-
  as_date(ymd(&quot;2020-01-01&quot;):(today() - 1))

scrape_dates %&gt;%
  map_dfr(
    .,
    ~ {
      print(now())
      Sys.sleep(10)
      write_csv(
        scrape_memeorandum(.),
        paste0(
          &quot;~/personal-website/content/post/memeorandum-scraping/data/memeorandum_&quot;,
          format(., &quot;%y%m%d&quot;),
          &quot;.csv&quot;
        )
      )
    }
  )</code></pre>
</div>
<div id="analysis" class="section level2">
<h2>Analysis</h2>
<p>Now that we have all the data in a <code>tibble</code>, we can start our analysis. Here's what the scraped data looks like:</p>
<pre class="r"><code>head(df) %&gt;% 
  knitr::kable()</code></pre>
<table>
<colgroup>
<col width="8%" />
<col width="12%" />
<col width="10%" />
<col width="62%" />
<col width="5%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">scrape_date</th>
<th align="left">outlet</th>
<th align="left">author</th>
<th align="left">headline</th>
<th align="left">section</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2020-01-01</td>
<td align="left">Alternet.org</td>
<td align="left">Alex Henderson</td>
<td align="left">GOP consultant says the Republican Party has reached a ‘day of reckoning’ as Trump consumes it whole</td>
<td align="left">sub</td>
</tr>
<tr class="even">
<td align="left">2020-01-01</td>
<td align="left">BBC</td>
<td align="left">NA</td>
<td align="left">US embassy attack: Protesters withdraw after standoff</td>
<td align="left">sub</td>
</tr>
<tr class="odd">
<td align="left">2020-01-01</td>
<td align="left">New York Times</td>
<td align="left">NA</td>
<td align="left">Pro_Iranian Protesters End Siege of U.S. Embassy in Baghdad</td>
<td align="left">sub</td>
</tr>
<tr class="even">
<td align="left">2020-01-01</td>
<td align="left">Quartz</td>
<td align="left">Justin Rohrlich</td>
<td align="left">Amid Baghdad embassy attacks, US spending on diplomatic security drops 11%</td>
<td align="left">sub</td>
</tr>
<tr class="odd">
<td align="left">2020-01-01</td>
<td align="left">Wall Street Journal</td>
<td align="left">NA</td>
<td align="left">Protesters Retreat From U.S. Embassy Site in Iraq</td>
<td align="left">sub</td>
</tr>
<tr class="even">
<td align="left">2020-01-01</td>
<td align="left">WND</td>
<td align="left">Joe Saunders</td>
<td align="left">Trump show of strength pays off, militants reported withdrawing from U.S. Embassy siege</td>
<td align="left">sub</td>
</tr>
</tbody>
</table>
<p>We can also use <code>tidytext::unnest_tokens()</code> to expand our data to the level of each individual word, and take out stop words while we're at it:</p>
<pre class="r"><code>data(stop_words)

df %&gt;% 
  slice(1) %&gt;% 
  unnest_tokens(word, headline) %&gt;% 
  anti_join(stop_words, by = &quot;word&quot;) %&gt;% 
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">scrape_date</th>
<th align="left">outlet</th>
<th align="left">author</th>
<th align="left">section</th>
<th align="left">word</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2020-01-01</td>
<td align="left">Alternet.org</td>
<td align="left">Alex Henderson</td>
<td align="left">sub</td>
<td align="left">gop</td>
</tr>
<tr class="even">
<td align="left">2020-01-01</td>
<td align="left">Alternet.org</td>
<td align="left">Alex Henderson</td>
<td align="left">sub</td>
<td align="left">consultant</td>
</tr>
<tr class="odd">
<td align="left">2020-01-01</td>
<td align="left">Alternet.org</td>
<td align="left">Alex Henderson</td>
<td align="left">sub</td>
<td align="left">republican</td>
</tr>
<tr class="even">
<td align="left">2020-01-01</td>
<td align="left">Alternet.org</td>
<td align="left">Alex Henderson</td>
<td align="left">sub</td>
<td align="left">party</td>
</tr>
<tr class="odd">
<td align="left">2020-01-01</td>
<td align="left">Alternet.org</td>
<td align="left">Alex Henderson</td>
<td align="left">sub</td>
<td align="left">reached</td>
</tr>
<tr class="even">
<td align="left">2020-01-01</td>
<td align="left">Alternet.org</td>
<td align="left">Alex Henderson</td>
<td align="left">sub</td>
<td align="left">day</td>
</tr>
<tr class="odd">
<td align="left">2020-01-01</td>
<td align="left">Alternet.org</td>
<td align="left">Alex Henderson</td>
<td align="left">sub</td>
<td align="left">reckoning</td>
</tr>
<tr class="even">
<td align="left">2020-01-01</td>
<td align="left">Alternet.org</td>
<td align="left">Alex Henderson</td>
<td align="left">sub</td>
<td align="left">trump</td>
</tr>
<tr class="odd">
<td align="left">2020-01-01</td>
<td align="left">Alternet.org</td>
<td align="left">Alex Henderson</td>
<td align="left">sub</td>
<td align="left">consumes</td>
</tr>
</tbody>
</table>
<p>To start off, let's look at the top eight topics over the course of the year. As we can see, Trump and his White House absolutely dominated coverage, as one might expect. No topic came close to matching the number of mentions he received, and he was mentioned at a steady pace throughout the year (though both he and Biden seem to have experienced a slight bump toward the end of the election cycle).</p>
<pre class="r"><code>top_8 &lt;-
  df %&gt;%
  unnest_tokens(word, headline) %&gt;%
  anti_join(stop_words, by = &quot;word&quot;) %&gt;%
  mutate(word = str_remove(word, &quot;\\&#39;s$&quot;)) %&gt;%
  count(word) %&gt;%
  slice_max(n = 8, order_by = n)

df %&gt;%
  unnest_tokens(word, headline) %&gt;%
  filter(word %in% top_8$word) %&gt;%
  group_by(word) %&gt;%
  arrange(scrape_date) %&gt;%
  mutate(row_n = row_number()) %&gt;%
  ungroup() %&gt;%
  ggplot(aes(scrape_date, row_n, color = word)) +
  geom_line(aes(group = word), show.legend = FALSE) +
  ggrepel::geom_label_repel(
    data = transmute(top_8, scrape_date = ymd(&quot;2020-12-13&quot;), row_n = n, word),
    aes(label = word),
    show.legend = FALSE
  ) +
  scale_y_continuous(labels = scales::comma) +
  theme_minimal() +
  theme(plot.title = element_markdown(face = &quot;bold&quot;)) +
  labs(
    x = NULL,
    y = &quot;Mentions&quot;,
    color = NULL,
    title = &quot;Most common _Memeorandum.com_ headline topics of 2020&quot;
  )</code></pre>
<p><img src="{{< relref "post/memeorandum-scraping/index.html" >}}index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Trump's dominance over the headlines was the rule, but there were exceptions. Here are all the days where Trump was <em>not</em> the most mentioned topic in the news. Non-Trump news cycles were far and few between, and most often emerged in the wake of the deaths of key figures like Kobe Bryant, George Floyd, John Lewis, and Ruth Bader Ginsburg.</p>
<pre class="r"><code># Top words over time
top_words &lt;-
  df %&gt;%
  unnest_tokens(word, headline) %&gt;%
  anti_join(stop_words, by = &quot;word&quot;) %&gt;%
  mutate(word = str_remove(word, &quot;\\&#39;s$&quot;)) %&gt;%
  count(scrape_date, word) %&gt;%
  group_by(scrape_date) %&gt;%
  slice_max(order_by = n, n = 1, with_ties = FALSE) %&gt;%
  ungroup()

top_words_text &lt;-
  top_words %&gt;%
  filter(word != &quot;trump&quot;) %&gt;%
  group_by(word) %&gt;%
  slice_min(order_by = scrape_date) %&gt;%
  ungroup() %&gt;%
  mutate(word = str_to_title(word))

top_words %&gt;%
  ggplot(aes(scrape_date, n)) +
  geom_col(aes(fill = word == &quot;trump&quot;), size = 0, show.legend = FALSE) +
  scale_fill_manual(values = c(&quot;red&quot;, &quot;grey&quot;)) +
  ggrepel::geom_label_repel(
    data = top_words_text,
    aes(label = word),
    vjust = 1,
    force = 20,
    arrow = arrow(angle = 45, length = unit(0.05, &quot;cm&quot;), ends = &quot;last&quot;, type = &quot;open&quot;)
  ) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    plot.title = element_markdown(face = &quot;bold&quot;),
    plot.subtitle = element_markdown()
  ) +
  labs(
    title = &quot;Which topics broke through Donald Trump&#39;s noise?&quot;,
    subtitle = &quot;Highlighting days &lt;span style = &#39;color:red;&#39;&gt;when Trump wasn&#39;t the #1 topic&lt;/span&gt; on _Memeorandum.com_.&quot;,
    y = &quot;Mentions&quot;,
    x = NULL
  )</code></pre>
<p><img src="{{< relref "post/memeorandum-scraping/index.html" >}}index_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>However, there was a long stretch between March and April where the focus turned elsewhere. March began with the end of the Democratic Primary — when Joe Biden won Super Tuesday and Elizabeth Warren dropped out — before quickly turning into a waking nightmare, as the coronavirus began to make its first show of force in America and sent parts of the country into lockdown.</p>
<pre class="r"><code>top_words &lt;-
  df %&gt;%
  filter(scrape_date &gt;= ymd(&quot;2020-03-01&quot;), scrape_date &lt; ymd(&quot;2020-05-01&quot;)) %&gt;% 
  unnest_tokens(word, headline) %&gt;%
  anti_join(stop_words, by = &quot;word&quot;) %&gt;%
  mutate(word = str_remove(word, &quot;\\&#39;s$&quot;)) %&gt;%
  count(scrape_date, word) %&gt;%
  group_by(scrape_date) %&gt;%
  slice_max(order_by = n, n = 1, with_ties = FALSE) %&gt;%
  ungroup()

top_words_text &lt;-
  top_words %&gt;%
  filter(word != &quot;trump&quot;) %&gt;%
  group_by(word) %&gt;%
  slice_min(order_by = scrape_date) %&gt;%
  ungroup() %&gt;%
  mutate(word = str_to_title(word))

top_words %&gt;%
  mutate(
    col = case_when(
      word == &quot;trump&quot; ~ &quot;Trump&quot;,
      word == &quot;coronavirus&quot; ~ &quot;COVID&quot;,
      word == &quot;warren&quot; ~ &quot;Warren&quot;,
      TRUE ~ &quot;Biden&quot;
    )
  ) %&gt;% 
  ggplot(aes(scrape_date, n)) +
  geom_col(aes(fill = col), size = 0) +
  scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;, &quot;grey&quot;, &quot;dodger blue&quot;)) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    plot.title = element_markdown(face = &quot;bold&quot;),
    plot.subtitle = element_markdown()
  ) +
  labs(
    title = &quot;Which topics broke through Donald Trump&#39;s noise?&quot;,
    subtitle = &quot;March through April, 2020&quot;,
    y = &quot;Mentions&quot;,
    x = NULL,
    fill = NULL
  )</code></pre>
<p><img src="{{< relref "post/memeorandum-scraping/index.html" >}}index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Inspired by the @<a href="https://twitter.com/NYT_first_said?s=20">NYT_first_said</a> bot on Twitter, which records the appearance of novel words in <em>The New York Times</em> lexicon, I wanted to see how often <em>new</em> words appeared on the main page — though for the purposes of this exercise, I'm only considering data from this year in my definition of &quot;new.&quot;</p>
<pre class="r"><code>## First appearances
first_appearances &lt;-
  df %&gt;%
  select(scrape_date, headline) %&gt;%
  unnest_tokens(word, headline) %&gt;%
  anti_join(stop_words, by = &quot;word&quot;) %&gt;%
  mutate(row_n = row_number()) %&gt;%
  group_by(scrape_date) %&gt;%
  mutate(row_n_date = row_number()) %&gt;%
  ungroup() %&gt;%
  group_by(word) %&gt;%
  mutate(first_appearance = row_n == min(row_n)) %&gt;%
  ungroup()

first_appearances %&gt;%
  ggplot(aes(scrape_date, -row_n_date, fill = first_appearance, alpha = first_appearance)) +
  geom_tile(show.legend = FALSE) +
  scale_fill_manual(values = c(&quot;grey&quot;, &quot;red&quot;)) + 
  scale_alpha_manual(values = c(0.15, 1)) + 
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    plot.title = element_markdown(face = &quot;bold&quot;),
    axis.text.y = element_blank()
  ) +
  labs(
    title = &quot;Prevalence of &lt;span style = &#39;color:red;&#39;&gt;new words&lt;/span&gt; on the _Memeorandum_ front page&quot;,
    y = NULL,
    x = NULL
  )</code></pre>
<p><img src="{{< relref "post/memeorandum-scraping/index.html" >}}index_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>I made this as more of a fun visual exercise than any sort of useful analysis, so in that spirit I'll top things off with a list of all the funky words that appeared on <em>Memeorandum</em> for the first time in 2020 on December 13th.</p>
<pre class="r"><code>first_appearances %&gt;%
  filter(scrape_date == ymd(&quot;2020-12-13&quot;), first_appearance) %&gt;%
  mutate(x = 0, y = 0) %&gt;%
  ggplot(aes(x, y, label = word)) +
  ggrepel::geom_label_repel(segment.alpha = 0, force = 15) +
  theme_void() +
  theme(plot.title = element_markdown(face = &quot;bold&quot;)) +
  labs(
    title = &quot;Words first appearing on _Memeorandum.com_ on 12/13/2020&quot;
  )</code></pre>
<p><img src="{{< relref "post/memeorandum-scraping/index.html" >}}index_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>There's plenty more to do with this data, so I may come back and update this post later. But for now I hope I've at least convinced you to check out <em>Memeorandum</em> for yourself!</p>
</div>
